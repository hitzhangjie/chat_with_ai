下面给出 ChatGPT 训练过程的文字版流程图，便于你直观理解各阶段的先后顺序与数据流向。
[预训练模型 GPT-3.5] --(大量无标签文本)--> [完成预训练]
            ↓
  (收集少量有监督指令数据)  --(SFT)-->  [监督微调模型]
            ↓
  (收集更多比较数据并人工打分)  --(RM)-->  [奖励模型]
            ↓
  (PPO 等强化学习算法)  --(RL)-->  [最终对齐模型]
            ↓
(可重复迭代：收集新比较数据 → 训练新奖励模型 → PPO 更新策略)

​​核心说明​​
1. ​​监督微调 (SFT)​​：在少量已标注的指令-响应对上继续训练，让模型先学会“按指令做事”。
2. ​​奖励模型 (RM)​​：对同一提示生成多条结果并排序，标注人员只需打分或排序，即可训练出能打分的奖励模型。
3. ​​强化学习 (RL)​​：把奖励模型作为环境信号，采用 PPO 等算法进行策略更新，使模型输出更符合人类偏好。
